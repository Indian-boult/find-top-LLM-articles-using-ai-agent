Title: Vintix: Action Model via In-Context Reinforcement Learning
Authors: Andrey Polubarov, Nikita Lyubaykin, Alexander Derevyagin, Ilya Zisman, Denis Tarasov, Alexander Nikulin, Vladislav Kurenkov
Abstract: In-Context Reinforcement Learning (ICRL) represents a promising paradigm for
developing generalist agents that learn at inference time through
trial-and-error interactions, analogous to how large language models adapt
contextually, but with a focus on reward maximization. However, the scalability
of ICRL beyond toy tasks and single-domain settings remains an open challenge.
In this work, we present the first steps toward scaling ICRL by introducing a
fixed, cross-domain model capable of learning behaviors through in-context
reinforcement learning. Our results demonstrate that Algorithm Distillation, a
framework designed to facilitate ICRL, offers a compelling and competitive
alternative to expert distillation to construct versatile action models. These
findings highlight the potential of ICRL as a scalable approach for generalist
decision-making systems. Code to be released at
https://github.com/dunnolab/vintix
arXiv ID: http://arxiv.org/abs/2501.19400v1
URL: http://arxiv.org/abs/2501.19400v1

================================================================================

Title: Do LLMs Strategically Reveal, Conceal, and Infer Information? A
  Theoretical and Empirical Analysis in The Chameleon Game
Authors: Mustafa O. Karabag, Ufuk Topcu
Abstract: Large language model-based (LLM-based) agents have become common in settings
that include non-cooperative parties. In such settings, agents' decision-making
needs to conceal information from their adversaries, reveal information to
their cooperators, and infer information to identify the other agents'
characteristics. To investigate whether LLMs have these information control and
decision-making capabilities, we make LLM agents play the language-based
hidden-identity game, The Chameleon. In the game, a group of non-chameleon
agents who do not know each other aim to identify the chameleon agent without
revealing a secret. The game requires the aforementioned information control
capabilities both as a chameleon and a non-chameleon. The empirical results
show that while non-chameleon LLM agents identify the chameleon, they fail to
conceal the secret from the chameleon, and their winning probability is far
from the levels of even trivial strategies. To formally explain this behavior,
we give a theoretical analysis for a spectrum of strategies, from concealing to
revealing, and provide bounds on the non-chameleons' winning probability. Based
on the empirical results and theoretical analysis of different strategies, we
deduce that LLM-based non-chameleon agents reveal excessive information to
agents of unknown identities. Our results point to a weakness of contemporary
LLMs, including GPT-4, GPT-4o, Gemini 1.5, and Claude 3.5 Sonnet, in strategic
interactions.
arXiv ID: http://arxiv.org/abs/2501.19398v1
URL: http://arxiv.org/abs/2501.19398v1

================================================================================

Title: Cache Me If You Must: Adaptive Key-Value Quantization for Large Language
  Models
Authors: Alina Shutova, Vladimir Malinovskii, Vage Egiazarian, Denis Kuznedelev, Denis Mazur, Nikita Surkov, Ivan Ermakov, Dan Alistarh
Abstract: Efficient real-world deployments of large language models (LLMs) rely on
Key-Value (KV) caching for processing and generating long outputs, reducing the
need for repetitive computation. For large contexts, Key-Value caches can take
up tens of gigabytes of device memory, as they store vector representations for
each token and layer. Recent work has shown that the cached vectors can be
compressed through quantization, pruning or merging, but these techniques often
compromise quality towards higher compression rates. In this work, we aim to
improve Key &amp; Value compression by exploiting two observations: 1) the inherent
dependencies between keys and values across different layers, and 2)
high-compression mechanisms for internal network states. We propose AQUA-KV, an
adaptive quantization for Key-Value caches that relies on compact adapters to
exploit existing dependencies between Keys and Values, and aims to "optimally"
compress the information that cannot be predicted. AQUA-KV significantly
improves compression rates, while maintaining high accuracy on state-of-the-art
LLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5
bits per value with under $1\%$ relative error in perplexity and LongBench
scores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a
single GPU within 1-6 hours, even for 70B models.
arXiv ID: http://arxiv.org/abs/2501.19392v1
URL: http://arxiv.org/abs/2501.19392v1

================================================================================

Title: Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large
  Language Models
Authors: Wenzhi Fang, Dong-Jun Han, Liangqi Yuan, Seyyedali Hosseinalipour, Christopher G. Brinton
Abstract: Fine-tuning large language models (LLMs) on devices is attracting increasing
interest. Recent works have fused low-rank adaptation (LoRA) techniques with
federated fine-tuning to mitigate challenges associated with device model sizes
and data scarcity. Still, the heterogeneity of computational resources remains
a critical bottleneck: while higher-rank modules generally enhance performance,
varying device capabilities constrain LoRA's feasible rank range. Existing
approaches attempting to resolve this issue either lack analytical
justification or impose additional computational overhead, leaving a wide gap
for an efficient and theoretically-grounded solution. To address these
challenges, we propose federated sketching LoRA (FSLoRA), which leverages a
sketching mechanism to enable devices to selectively update submatrices of
global LoRA modules maintained by the server. By adjusting the sketching
ratios, which determine the ranks of the submatrices on the devices, FSLoRA
flexibly adapts to device-specific communication and computational constraints.
We provide a rigorous convergence analysis of FSLoRA that characterizes how the
sketching ratios affect the convergence rate. Through comprehensive experiments
on multiple datasets and LLM models, we demonstrate FSLoRA's superior
performance compared to various baselines.
arXiv ID: http://arxiv.org/abs/2501.19389v1
URL: http://arxiv.org/abs/2501.19389v1

================================================================================

Title: SELMA: A Speech-Enabled Language Model for Virtual Assistant
  Interactions
Authors: Dominik Wagner, Alexander Churchill, Siddarth Sigtia, Erik Marchi
Abstract: In this work, we present and evaluate SELMA, a Speech-Enabled Language Model
for virtual Assistant interactions that integrates audio and text as inputs to
a Large Language Model (LLM). SELMA is designed to handle three primary and two
auxiliary tasks related to interactions with virtual assistants simultaneously
within a single end-to-end model. We employ low-rank adaptation modules for
parameter-efficient training of both the audio encoder and the LLM.
Additionally, we implement a feature pooling strategy enabling the system to
recognize global patterns and improve accuracy on tasks less reliant on
individual sequence elements. Experimental results on Voice Trigger (VT)
detection, Device-Directed Speech Detection (DDSD), and Automatic Speech
Recognition (ASR), demonstrate that our approach both simplifies the typical
input processing pipeline of virtual assistants significantly and also improves
performance compared to dedicated models for each individual task. SELMA yields
relative Equal-Error Rate improvements of 64% on the VT detection task, and 22%
on DDSD, while also achieving word error rates close to the baseline.
arXiv ID: http://arxiv.org/abs/2501.19377v1
URL: http://arxiv.org/abs/2501.19377v1

================================================================================

Title: We're Different, We're the Same: Creative Homogeneity Across LLMs
Authors: Emily Wenger, Yoed Kenett
Abstract: Numerous powerful large language models (LLMs) are now available for use as
writing support tools, idea generators, and beyond. Although these LLMs are
marketed as helpful creative assistants, several works have shown that using an
LLM as a creative partner results in a narrower set of creative outputs.
However, these studies only consider the effects of interacting with a single
LLM, begging the question of whether such narrowed creativity stems from using
a particular LLM -- which arguably has a limited range of outputs -- or from
using LLMs in general as creative assistants. To study this question, we elicit
creative responses from humans and a broad set of LLMs using standardized
creativity tests and compare the population-level diversity of responses. We
find that LLM responses are much more similar to other LLM responses than human
responses are to each other, even after controlling for response structure and
other key variables. This finding of significant homogeneity in creative
outputs across the LLMs we evaluate adds a new dimension to the ongoing
conversation about creativity and LLMs. If today's LLMs behave similarly, using
them as a creative partners -- regardless of the model used -- may drive all
users towards a limited set of "creative" outputs.
arXiv ID: http://arxiv.org/abs/2501.19361v1
URL: http://arxiv.org/abs/2501.19361v1

================================================================================

Title: Mechanical Properties of the Meninges: Large Language Model Assisted
  Systematic Review of over 25,000 Studies
Authors: Brandon P. Chelstrom, Maciej P. Polak, Dane Morgan, Corinne R. Henak
Abstract: Accurate constitutive models and corresponding mechanical property values for
the meninges are important for predicting mechanical damage to brain tissue due
to traumatic brain injury. The meninges are often oversimplified in current
finite element (FE) head models due to their complex anatomy and
spatially-variant mechanical behavior. This study performed a systematic review
(SR) on the mechanical properties of each individual layer of the meninges to
obtain benchmark data for FE modeling and to identify gaps in the current
literature. Relevant studies were filtered through three stages: a broad
initial search filter, a large language model classifier, and manual
verification by a human reviewer. Out of over 25,000 studies initially
considered, this review ultimately included 47 studies on the dura mater, 8 on
the arachnoid mater, and 7 on the pia mater, representing the largest and most
comprehensive SR on the mechanical properties of the meninges. Each layer was
found to exhibit nonlinear rate dependence that varies with species, age,
location, and orientation. This study revealed that the elastic modulus of pia
mater most often used in simplified linear elastic FE models is likely
underestimated by an order of magnitude and fails to consider directional
dependence. Future studies investigating the mechanical properties of the
meninges should focus on a wider range of loading rates as well as age effects
for the arachnoid mater and pia mater, as these features are relatively
understudied and expected to affect the fidelity of FE predictions.
arXiv ID: http://arxiv.org/abs/2501.19359v1
URL: http://arxiv.org/abs/2501.19359v1

================================================================================

Title: The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating
  Reward Hacking
Authors: Yuchun Miao, Sen Zhang, Liang Ding, Yuqi Zhang, Lefei Zhang, Dacheng Tao
Abstract: This work identifies the Energy Loss Phenomenon in Reinforcement Learning
from Human Feedback (RLHF) and its connection to reward hacking. Specifically,
energy loss in the final layer of a Large Language Model (LLM) gradually
increases during the RL process, with an excessive increase in energy loss
characterizing reward hacking. Beyond empirical analysis, we further provide a
theoretical foundation by proving that, under mild conditions, the increased
energy loss reduces the upper bound of contextual relevance in LLMs, which is a
critical aspect of reward hacking as the reduced contextual relevance typically
indicates overfitting to reward model-favored patterns in RL. To address this
issue, we propose an Energy loss-aware PPO algorithm (EPPO) which penalizes the
increase in energy loss in the LLM's final layer during reward calculation to
prevent excessive energy loss, thereby mitigating reward hacking. We
theoretically show that EPPO can be conceptually interpreted as an
entropy-regularized RL algorithm, which provides deeper insights into its
effectiveness. Extensive experiments across various LLMs and tasks demonstrate
the commonality of the energy loss phenomenon, as well as the effectiveness of
\texttt{EPPO} in mitigating reward hacking and improving RLHF performance.
arXiv ID: http://arxiv.org/abs/2501.19358v1
URL: http://arxiv.org/abs/2501.19358v1

================================================================================

Title: Towards Adaptive Self-Improvement for Smarter Energy Systems
Authors: Alexander Sommer, Peter Bazan, Jonathan Fellerer, Behnam Babaeian, Reinhard German
Abstract: This paper introduces a hierarchical framework for decision-making and
optimization, leveraging Large Language Models (LLMs) for adaptive code
generation. Instead of direct decision-making, LLMs generate and refine
executable control policies through a meta-policy that guides task generation
and a base policy for operational actions. Applied to a simplified microgrid
scenario, the approach achieves up to 15 percent cost savings by iteratively
improving battery control strategies. The proposed methodology lays a
foundation for integrating LLM-based tools into planning and control tasks,
offering adaptable and scalable solutions for complex systems while addressing
challenges of uncertainty and reproducibility.
arXiv ID: http://arxiv.org/abs/2501.19340v1
URL: http://arxiv.org/abs/2501.19340v1

================================================================================

Title: Homogeneity Bias as Differential Sampling Uncertainty in Language Models
Authors: Messi H. J. Lee, Soyeon Jeon
Abstract: Prior research show that Large Language Models (LLMs) and Vision-Language
Models (VLMs) represent marginalized groups more homogeneously than dominant
groups. However, the mechanisms underlying this homogeneity bias remain
relatively unexplored. We propose that this bias emerges from systematic
differences in the probability distributions from which tokens are sampled at
inference-time. Analyzing three measures of uncertainty in token sampling
distributions-entropy, perplexity, and probability of differentiation-we find
that in some models, specifically GPT-4 Turbo and Llama-3.2, tokens are sampled
more deterministically when generating texts about marginalized groups (i.e.,
Black Americans and women) compared to their dominant group counterparts (i.e.,
White Americans and men). While these findings may help explain homogeneity
bias in certain models, the patterns did not replicate across all VLMs tested,
suggesting multiple mechanisms may contribute to homogeneity bias in AI.
arXiv ID: http://arxiv.org/abs/2501.19337v1
URL: http://arxiv.org/abs/2501.19337v1

================================================================================

Title: Reward-Guided Speculative Decoding for Efficient LLM Reasoning
Authors: Baohao Liao, Yuhui Xu, Hanze Dong, Junnan Li, Christof Monz, Silvio Savarese, Doyen Sahoo, Caiming Xiong
Abstract: We introduce Reward-Guided Speculative Decoding (RSD), a novel framework
aimed at improving the efficiency of inference in large language models (LLMs).
RSD synergistically combines a lightweight draft model with a more powerful
target model, incorporating a controlled bias to prioritize high-reward
outputs, in contrast to existing speculative decoding methods that enforce
strict unbiasedness. RSD employs a process reward model to evaluate
intermediate decoding steps and dynamically decide whether to invoke the target
model, optimizing the trade-off between computational cost and output quality.
We theoretically demonstrate that a threshold-based mixture strategy achieves
an optimal balance between resource utilization and performance. Extensive
evaluations on challenging reasoning benchmarks, including Olympiad-level
tasks, show that RSD delivers significant efficiency gains against decoding
with the target model only (up to 4.4x fewer FLOPs), while achieving
significant better accuracy than parallel decoding method on average (up to
+3.5). These results highlight RSD as a robust and cost-effective approach for
deploying LLMs in resource-intensive scenarios.
arXiv ID: http://arxiv.org/abs/2501.19324v1
URL: http://arxiv.org/abs/2501.19324v1

================================================================================

Title: MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented
  Reinforcement in Embodied Systems
Authors: Anirudh Chari, Suraj Reddy, Aditya Tiwari, Richard Lian, Brian Zhou
Abstract: While large language models (LLMs) have shown promising capabilities as
zero-shot planners for embodied agents, their inability to learn from
experience and build persistent mental models limits their robustness in
complex open-world environments like Minecraft. We introduce MINDSTORES, an
experience-augmented planning framework that enables embodied agents to build
and leverage mental models through natural interaction with their environment.
Drawing inspiration from how humans construct and refine cognitive mental
models, our approach extends existing zero-shot LLM planning by maintaining a
database of past experiences that informs future planning iterations. The key
innovation is representing accumulated experiences as natural language
embeddings of (state, task, plan, outcome) tuples, which can then be
efficiently retrieved and reasoned over by an LLM planner to generate insights
and guide plan refinement for novel states and tasks. Through extensive
experiments in the MineDojo environment, a simulation environment for agents in
Minecraft that provides low-level controls for Minecraft, we find that
MINDSTORES learns and applies its knowledge significantly better than existing
memory-based LLM planners while maintaining the flexibility and generalization
benefits of zero-shot approaches, representing an important step toward more
capable embodied AI systems that can learn continuously through natural
experience.
arXiv ID: http://arxiv.org/abs/2501.19318v1
URL: http://arxiv.org/abs/2501.19318v1

================================================================================

Title: LLM-based Affective Text Generation Quality Based on Different
  Quantization Values
Authors: Yarik Menchaca Resendiz, Roman Klinger
Abstract: Large language models exhibit a remarkable capacity in language generation
and comprehension. These advances enable AI systems to produce more human-like
and emotionally engaging text. However, these models rely on a large number of
parameters, requiring significant computational resources for training and
inference. In some scenarios, accessing these resources can be challenging
(e.g., budget or hardware limitations). Techniques like reducing precision bits
can make models more memory-efficient, reducing the computational resources
needed, at the cost of reduced accuracy. This paper addresses the trade-off
between different quantization values, GPU RAM utilization, and text quality in
affective text generation (e.g., "I really enjoy running in the snow-covered
forest"). To evaluate, we use an emotion classifier and ten seed prompts to
generate affective text. We test three setups of precision bits (8, 16, and 32)
across five open-weight language models from two different families. Our
findings demonstrate that bit reductions lead to memory savings, achieving a
reduction of 76%. However, this optimization comes with a trade-off, leading to
a decrease of up to 10 pp in F1 score for larger models and an increase of 10
pp for smaller models, along with roughly double the inference time. In terms
of text quality, larger models at lower quantization levels generally
outperform smaller, higher-precision models -- while requiring similar memory.
arXiv ID: http://arxiv.org/abs/2501.19317v1
URL: http://arxiv.org/abs/2501.19317v1

================================================================================

Title: Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model
  Alignment
Authors: Gregor Bachmann, Sotiris Anagnostidis, Albert Pumarola, Markos Georgopoulos, Artsiom Sanakoyeu, Yuming Du, Edgar Schönfeld, Ali Thabet, Jonas Kohler
Abstract: The performance of large language models (LLMs) is closely linked to their
underlying size, leading to ever-growing networks and hence slower inference.
Speculative decoding has been proposed as a technique to accelerate
autoregressive generation, leveraging a fast draft model to propose candidate
tokens, which are then verified in parallel based on their likelihood under the
target model. While this approach guarantees to reproduce the target output, it
incurs a substantial penalty: many high-quality draft tokens are rejected, even
when they represent objectively valid continuations. Indeed, we show that even
powerful draft models such as GPT-4o, as well as human text cannot achieve high
acceptance rates under the standard verification scheme. This severely limits
the speedup potential of current speculative decoding methods, as an early
rejection becomes overwhelmingly likely when solely relying on alignment of
draft and target.
  We thus ask the following question: Can we adapt verification to recognize
correct, but non-aligned replies? To this end, we draw inspiration from the
LLM-as-a-judge framework, which demonstrated that LLMs are able to rate answers
in a versatile way. We carefully design a dataset to elicit the same capability
in the target model by training a compact module on top of the embeddings to
produce ``judgements" of the current continuation. We showcase our strategy on
the Llama-3.1 family, where our 8b/405B-Judge achieves a speedup of 9x over
Llama-405B, while maintaining its quality on a large range of benchmarks. These
benefits remain present even in optimized inference frameworks, where our
method reaches up to 141 tokens/s for 8B/70B-Judge and 129 tokens/s for 8B/405B
on 2 and 8 H100s respectively.
arXiv ID: http://arxiv.org/abs/2501.19309v1
URL: http://arxiv.org/abs/2501.19309v1

================================================================================

Title: SETS: Leveraging Self-Verification and Self-Correction for Improved
  Test-Time Scaling
Authors: Jiefeng Chen, Jie Ren, Xinyun Chen, Chengrun Yang, Ruoxi Sun, Sercan Ö Arık
Abstract: Recent advancements in Large Language Models (LLMs) have created new
opportunities to enhance performance on complex reasoning tasks by leveraging
test-time computation. However, conventional approaches such as repeated
sampling with majority voting or reward model scoring, often face diminishing
returns as test-time compute scales, in addition to requiring costly
task-specific reward model training. In this paper, we present Self-Enhanced
Test-Time Scaling (SETS), a novel method that leverages the self-verification
and self-correction capabilities of recent advanced LLMs to overcome these
limitations. SETS integrates sampling, self-verification, and self-correction
into a unified framework, enabling efficient and scalable test-time computation
for improved capabilities at complex tasks. Through extensive experiments on
challenging planning and reasoning benchmarks, compared to the alternatives, we
demonstrate that SETS achieves significant performance improvements and more
favorable test-time scaling laws.
arXiv ID: http://arxiv.org/abs/2501.19306v1
URL: http://arxiv.org/abs/2501.19306v1

================================================================================

Title: Beyond checkmate: exploring the creative chokepoints in AI text
Authors: Nafis Irtiza Tripto, Saranya Venkatraman, Mahjabin Nahar, Dongwon Lee
Abstract: Large Language Models (LLMs) have revolutionized Natural Language Processing
(NLP) and Artificial Intelligence (AI), unlocking unprecedented capabilities.
This rapid advancement has spurred research into various aspects of LLMs, their
text generation &amp; reasoning capability, and potential misuse, fueling the
necessity for robust detection methods. While numerous prior research has
focused on detecting LLM-generated text (AI text) and thus checkmating them,
our study investigates a relatively unexplored territory: portraying the
nuanced distinctions between human and AI texts across text segments. Whether
LLMs struggle with or excel at incorporating linguistic ingenuity across
different text segments carries substantial implications for determining their
potential as effective creative assistants to humans. Through an analogy with
the structure of chess games-comprising opening, middle, and end games-we
analyze text segments (introduction, body, and conclusion) to determine where
the most significant distinctions between human and AI texts exist. While AI
texts can approximate the body segment better due to its increased length, a
closer examination reveals a pronounced disparity, highlighting the importance
of this segment in AI text detection. Additionally, human texts exhibit higher
cross-segment differences compared to AI texts. Overall, our research can shed
light on the intricacies of human-AI text distinctions, offering novel insights
for text detection and understanding.
arXiv ID: http://arxiv.org/abs/2501.19301v1
URL: http://arxiv.org/abs/2501.19301v1

================================================================================

Title: Offline Learning for Combinatorial Multi-armed Bandits
Authors: Xutong Liu, Xiangxiang Dai, Jinhang Zuo, Siwei Wang, Carlee-Joe Wong, John C. S. Lui, Wei Chen
Abstract: The combinatorial multi-armed bandit (CMAB) is a fundamental sequential
decision-making framework, extensively studied over the past decade. However,
existing work primarily focuses on the online setting, overlooking the
substantial costs of online interactions and the readily available offline
datasets. To overcome these limitations, we introduce Off-CMAB, the first
offline learning framework for CMAB. Central to our framework is the
combinatorial lower confidence bound (CLCB) algorithm, which combines
pessimistic reward estimations with combinatorial solvers. To characterize the
quality of offline datasets, we propose two novel data coverage conditions and
prove that, under these conditions, CLCB achieves a near-optimal suboptimality
gap, matching the theoretical lower bound up to a logarithmic factor. We
validate Off-CMAB through practical applications, including learning to rank,
large language model (LLM) caching, and social influence maximization, showing
its ability to handle nonlinear reward functions, general feedback models, and
out-of-distribution action samples that excludes optimal or even feasible
actions. Extensive experiments on synthetic and real-world datasets further
highlight the superior performance of CLCB.
arXiv ID: http://arxiv.org/abs/2501.19300v1
URL: http://arxiv.org/abs/2501.19300v1

================================================================================

Title: Synthetic User Behavior Sequence Generation with Large Language Models
  for Smart Homes
Authors: Zhiyao Xu, Dan Zhao, Qingsong Zou, Jingyu Xiao, Yong Jiang, Zhenhui Yuan, Qing Li
Abstract: In recent years, as smart home systems have become more widespread, security
concerns within these environments have become a growing threat. Currently,
most smart home security solutions, such as anomaly detection and behavior
prediction models, are trained using fixed datasets that are precollected.
However, the process of dataset collection is time-consuming and lacks the
flexibility needed to adapt to the constantly evolving smart home environment.
Additionally, the collection of personal data raises significant privacy
concerns for users. Lately, large language models (LLMs) have emerged as a
powerful tool for a wide range of tasks across diverse application domains,
thanks to their strong capabilities in natural language processing, reasoning,
and problem-solving. In this paper, we propose an LLM-based synthetic dataset
generation IoTGen framework to enhance the generalization of downstream smart
home intelligent models. By generating new synthetic datasets that reflect
changes in the environment, smart home intelligent models can be retrained to
overcome the limitations of fixed and outdated data, allowing them to better
align with the dynamic nature of real-world home environments. Specifically, we
first propose a Structure Pattern Perception Compression (SPPC) method tailored
for IoT behavior data, which preserves the most informative content in the data
while significantly reducing token consumption. Then, we propose a systematic
approach to create prompts and implement data generation to automatically
generate IoT synthetic data with normative and reasonable properties, assisting
task models in adaptive training to improve generalization and real-world
performance.
arXiv ID: http://arxiv.org/abs/2501.19298v1
URL: http://arxiv.org/abs/2501.19298v1

================================================================================

Title: Analysis of LLMs vs Human Experts in Requirements Engineering
Authors: Cory Hymel, Hiroe Johnson
Abstract: The majority of research around Large Language Models (LLM) application to
software development has been on the subject of code generation. There is
little literature on LLMs' impact on requirements engineering (RE), which deals
with the process of developing and verifying the system requirements. Within
RE, there is a subdiscipline of requirements elicitation, which is the practice
of discovering and documenting requirements for a system from users, customers,
and other stakeholders. In this analysis, we compare LLM's ability to elicit
requirements of a software system, as compared to that of a human expert in a
time-boxed and prompt-boxed study. We found LLM-generated requirements were
evaluated as more aligned (+1.12) than human-generated requirements with a
trend of being more complete (+10.2%). Conversely, we found users tended to
believe that solutions they perceived as more aligned had been generated by
human experts. Furthermore, while LLM-generated documents scored higher and
performed at 720x the speed, their cost was, on average, only 0.06% that of a
human expert. Overall, these findings indicate that LLMs will play an
increasingly important role in requirements engineering by improving
requirements definitions, enabling more efficient resource allocation, and
reducing overall project timelines.
arXiv ID: http://arxiv.org/abs/2501.19297v1
URL: http://arxiv.org/abs/2501.19297v1

================================================================================

Title: Differentially Private In-context Learning via Sampling Few-shot Mixed
  with Zero-shot Outputs
Authors: James Flemings, Haosheng Gan, Hongyi Li, Meisam Razaviyayn, Murali Annavaram
Abstract: In-context learning (ICL) has shown promising improvement in downstream task
adaptation of LLMs by augmenting prompts with relevant input-output examples
(demonstrations). However, the ICL demonstrations can contain privacy-sensitive
information, which can be leaked and/or regurgitated by the LLM output.
Differential Privacy (DP), a widely adopted privacy safeguard, has emerged to
mitigate this privacy leakage, with recent work demonstrating strong
privacy-utility tradeoffs in classification tasks for ICL. However, generation
tasks for ICL are challenging due to the high-dimensional output space of
open-ended generation. To this end, we propose $\texttt{dps-mozo}$,
Differentially Private Sampling by Mixing One-shot with Zero-shot Outputs, a
decoding framework that generates DP text by sampling from the product of
multiple one-shot outputs mixed with a zero-shot output. This mixing
effectively reduces the amount of information that can be leaked by each
demonstration. By utilizing the inherent randomness in sampling from the mixed
distributions, we can achieve DP without adding noise, thereby improving the
privacy-utility tradeoff. Our experimental evaluations show $\texttt{dps-mozo}$
can achieve a strong privacy guarantee, $\epsilon=2$, with minimal utility
degradation compared to non-private few-shot learning, $\textbf{0.3}$% ROUGE-L
F1 score decrease on the SAMSum dataset with Gemma 2 2B.
arXiv ID: http://arxiv.org/abs/2501.19287v1
URL: http://arxiv.org/abs/2501.19287v1

================================================================================

Title: Low-Cost and Comprehensive Non-textual Input Fuzzing with
  LLM-Synthesized Input Generators
Authors: Kunpeng Zhang, Zongjie Li, Daoyuan Wu, Shuai Wang, Xin Xia
Abstract: Modern software often accepts inputs with highly complex grammars. Recent
advances in large language models (LLMs) have shown that they can be used to
synthesize high-quality natural language text and code that conforms to the
grammar of a given input format. Nevertheless, LLMs are often incapable or too
costly to generate non-textual outputs, such as images, videos, and PDF files.
This limitation hinders the application of LLMs in grammar-aware fuzzing.
  We present a novel approach to enabling grammar-aware fuzzing over
non-textual inputs. We employ LLMs to synthesize and also mutate input
generators, in the form of Python scripts, that generate data conforming to the
grammar of a given input format. Then, non-textual data yielded by the input
generators are further mutated by traditional fuzzers (AFL++) to explore the
software input space effectively. Our approach, namely G2FUZZ, features a
hybrid strategy that combines a holistic search driven by LLMs and a local
search driven by industrial quality fuzzers. Two key advantages are: (1) LLMs
are good at synthesizing and mutating input generators and enabling jumping out
of local optima, thus achieving a synergistic effect when combined with
mutation-based fuzzers; (2) LLMs are less frequently invoked unless really
needed, thus significantly reducing the cost of LLM usage. We have evaluated
G2FUZZ on a variety of input formats, including TIFF images, MP4 audios, and
PDF files. The results show that G2FUZZ outperforms SOTA tools such as AFL++,
Fuzztruction, and FormatFuzzer in terms of code coverage and bug finding across
most programs tested on three platforms: UNIFUZZ, FuzzBench, and MAGMA.
arXiv ID: http://arxiv.org/abs/2501.19282v1
URL: http://arxiv.org/abs/2501.19282v1

================================================================================

Title: Pheromone-based Learning of Optimal Reasoning Paths
Authors: Anirudh Chari, Aditya Tiwari, Richard Lian, Suraj Reddy, Brian Zhou
Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning
capabilities through chain-of-thought prompting, yet discovering effective
reasoning methods for complex problems remains challenging due to the vast
space of possible intermediate steps. We introduce Ant Colony
Optimization-guided Tree of Thought (ACO-ToT), a novel algorithm that combines
ACO with LLMs to discover optimal reasoning paths for complex problems
efficiently. Drawing inspiration from Hebbian learning in neurological systems,
our method employs a collection of distinctly fine-tuned LLM "ants" to traverse
and lay pheromone trails through a centralized tree of thought, with each ant's
movement governed by a weighted combination of existing pheromone trails and
its own specialized expertise. The algorithm evaluates complete reasoning paths
using a mixture-of-experts-based scoring function, with pheromones reinforcing
productive reasoning paths across iterations. Experiments on three challenging
reasoning tasks (GSM8K, ARC-Challenge, and MATH) demonstrate that ACO-ToT
performs significantly better than existing chain-of-thought optimization
approaches, suggesting that incorporating biologically inspired collective
search mechanisms into LLM inference can substantially enhance reasoning
capabilities.
arXiv ID: http://arxiv.org/abs/2501.19278v1
URL: http://arxiv.org/abs/2501.19278v1

================================================================================

Title: From Assistance to Autonomy -- A Researcher Study on the Potential of AI
  Support for Qualitative Data Analysis
Authors: Elisabeth Kirsten, Annalina Buckmann, Leona Lassak, Nele Borgert, Abraham Mhaidli, Steffen Becker
Abstract: The advent of AI tools, such as Large Language Models, has introduced new
possibilities for Qualitative Data Analysis (QDA), offering both opportunities
and challenges. To help navigate the responsible integration of AI into QDA, we
conducted semi-structured interviews with 15 HCI researchers experienced in
QDA. While our participants were open to AI support in their QDA workflows,
they expressed concerns about data privacy, autonomy, and the quality of AI
outputs. In response, we developed a framework that spans from minimal to high
AI involvement, providing tangible scenarios for integrating AI into HCI
researchers' QDA practices while addressing their needs and concerns. Aligned
with real-life QDA workflows, we identify potentials for AI tools in areas such
as data pre-processing, researcher onboarding, or mediation. Our framework aims
to provoke further discussion on the development of AI-supported QDA and to
help establish community standards for their responsible use.
arXiv ID: http://arxiv.org/abs/2501.19275v1
URL: http://arxiv.org/abs/2501.19275v1

================================================================================

Title: Jackpot! Alignment as a Maximal Lottery
Authors: Roberto-Rafael Maura-Rivero, Marc Lanctot, Francesco Visin, Kate Larson
Abstract: Reinforcement Learning from Human Feedback (RLHF), the standard for aligning
Large Language Models (LLMs) with human values, is known to fail to satisfy
properties that are intuitively desirable, such as respecting the preferences
of the majority \cite{ge2024axioms}. To overcome these issues, we propose the
use of a probabilistic Social Choice rule called \emph{maximal lotteries} as a
replacement for RLHF. We show that a family of alignment techniques, namely
Nash Learning from Human Feedback (NLHF) \cite{munos2023nash} and variants,
approximate maximal lottery outcomes and thus inherit its beneficial
properties.
  We confirm experimentally that our proposed methodology handles situations
that arise when working with preferences more robustly than standard RLHF,
including supporting the preferences of the majority, providing principled ways
of handling non-transitivities in the preference data, and robustness to
irrelevant alternatives. This results in systems that better incorporate human
values and respect human intentions.
arXiv ID: http://arxiv.org/abs/2501.19266v1
URL: http://arxiv.org/abs/2501.19266v1

================================================================================

Title: Neuro-LIFT: A Neuromorphic, LLM-based Interactive Framework for
  Autonomous Drone FlighT at the Edge
Authors: Amogh Joshi, Sourav Sanyal, Kaushik Roy
Abstract: The integration of human-intuitive interactions into autonomous systems has
been limited. Traditional Natural Language Processing (NLP) systems struggle
with context and intent understanding, severely restricting human-robot
interaction. Recent advancements in Large Language Models (LLMs) have
transformed this dynamic, allowing for intuitive and high-level communication
through speech and text, and bridging the gap between human commands and
robotic actions. Additionally, autonomous navigation has emerged as a central
focus in robotics research, with artificial intelligence (AI) increasingly
being leveraged to enhance these systems. However, existing AI-based navigation
algorithms face significant challenges in latency-critical tasks where rapid
decision-making is critical. Traditional frame-based vision systems, while
effective for high-level decision-making, suffer from high energy consumption
and latency, limiting their applicability in real-time scenarios. Neuromorphic
vision systems, combining event-based cameras and spiking neural networks
(SNNs), offer a promising alternative by enabling energy-efficient, low-latency
navigation. Despite their potential, real-world implementations of these
systems, particularly on physical platforms such as drones, remain scarce. In
this work, we present Neuro-LIFT, a real-time neuromorphic navigation framework
implemented on a Parrot Bebop2 quadrotor. Leveraging an LLM for natural
language processing, Neuro-LIFT translates human speech into high-level
planning commands which are then autonomously executed using event-based
neuromorphic vision and physics-driven planning. Our framework demonstrates its
capabilities in navigating in a dynamic environment, avoiding obstacles, and
adapting to human instructions in real-time.
arXiv ID: http://arxiv.org/abs/2501.19259v1
URL: http://arxiv.org/abs/2501.19259v1

================================================================================

Title: A Zero-Shot Generalization Framework for LLM-Driven Cross-Domain
  Sequential Recommendation
Authors: Yunzhe Li, Junting Wang, Hari Sundaram, Zhining Liu
Abstract: Zero-shot cross-domain sequential recommendation (ZCDSR) enables predictions
in unseen domains without the need for additional training or fine-tuning,
making it particularly valuable in data-sparse environments where traditional
models struggle. Recent advancements in large language models (LLMs) have
greatly improved ZCDSR by leveraging rich pretrained representations to
facilitate cross-domain knowledge transfer. However, a key challenge persists:
domain semantic bias, which arises from variations in vocabulary and content
focus across domains. This misalignment leads to inconsistencies in item
embeddings and hinders generalization.
  To address this issue, we propose a novel framework designed to enhance
LLM-based ZCDSR by improving cross-domain alignment at both the item and
sequential levels. At the item level, we introduce a generalization loss that
promotes inter-domain compactness by aligning embeddings of similar items
across domains while maintaining intra-domain diversity to preserve unique item
characteristics. This prevents embeddings from becoming overly generic while
ensuring effective transferability. At the sequential level, we develop a
method for transferring user behavioral patterns by clustering user sequences
in the source domain and applying attention-based aggregation for target domain
inference. This dynamic adaptation of user embeddings allows effective
zero-shot recommendations without requiring target-domain interactions.
  Comprehensive experiments across multiple datasets and domains demonstrate
that our framework significantly improves sequential recommendation performance
in the ZCDSR setting. By mitigating domain bias and enhancing the
transferability of sequential patterns, our method provides a scalable and
robust approach for achieving more effective zero-shot recommendations across
domains.
arXiv ID: http://arxiv.org/abs/2501.19232v1
URL: http://arxiv.org/abs/2501.19232v1

================================================================================

Title: Through the Looking Glass: LLM-Based Analysis of AR/VR Android
  Applications Privacy Policies
Authors: Abdulaziz Alghamdi, David Mohaisen
Abstract: \begin{abstract} This paper comprehensively analyzes privacy policies in
AR/VR applications, leveraging BERT, a state-of-the-art text classification
model, to evaluate the clarity and thoroughness of these policies. By comparing
the privacy policies of AR/VR applications with those of free and premium
websites, this study provides a broad perspective on the current state of
privacy practices within the AR/VR industry. Our findings indicate that AR/VR
applications generally offer a higher percentage of positive segments than free
content but lower than premium websites. The analysis of highlighted segments
and words revealed that AR/VR applications strategically emphasize critical
privacy practices and key terms. This enhances privacy policies' clarity and
effectiveness.
arXiv ID: http://arxiv.org/abs/2501.19223v1
URL: http://arxiv.org/abs/2501.19223v1

================================================================================

Title: Autonomous Legacy Web Application Upgrades Using a Multi-Agent System
Authors: Valtteri Ala-Salmi, Zeeshan Rasheed, Abdul Malik Sami, Zheying Zhang, Kai-Kristian Kemell, Jussi Rasku, Shahbaz Siddeeq, Mika Saari, Pekka Abrahamsson
Abstract: The use of Large Language Models (LLMs) for autonomous code generation is
gaining attention in emerging technologies. As LLM capabilities expand, they
offer new possibilities such as code refactoring, security enhancements, and
legacy application upgrades. Many outdated web applications pose security and
reliability challenges, yet companies continue using them due to the complexity
and cost of upgrades. To address this, we propose an LLM-based multi-agent
system that autonomously upgrades legacy web applications to the latest
versions. The system distributes tasks across multiple phases, updating all
relevant files. To evaluate its effectiveness, we employed Zero-Shot Learning
(ZSL) and One-Shot Learning (OSL) prompts, applying identical instructions in
both cases. The evaluation involved updating view files and measuring the
number and types of errors in the output. For complex tasks, we counted the
successfully met requirements. The experiments compared the proposed system
with standalone LLM execution, repeated multiple times to account for
stochastic behavior. Results indicate that our system maintains context across
tasks and agents, improving solution quality over the base model in some cases.
This study provides a foundation for future model implementations in legacy
code updates. Additionally, findings highlight LLMs' ability to update small
outdated files with high precision, even with basic prompts. The source code is
publicly available on GitHub: https://github.com/alasalm1/Multi-agent-pipeline.
arXiv ID: http://arxiv.org/abs/2501.19204v1
URL: http://arxiv.org/abs/2501.19204v1

================================================================================

Title: Improving the Robustness of Representation Misdirection for Large
  Language Model Unlearning
Authors: Dang Huu-Tien, Hoang Thanh-Tung, Le-Minh Nguyen, Naoya Inoue
Abstract: Representation Misdirection (RM) and variants are established large language
model (LLM) unlearning methods with state-of-the-art performance. In this
paper, we show that RM methods inherently reduce models' robustness, causing
them to misbehave even when a single non-adversarial forget-token is in the
retain-query. Toward understanding underlying causes, we reframe the unlearning
process as backdoor attacks and defenses: forget-tokens act as backdoor
triggers that, when activated in retain-queries, cause disruptions in RM
models' behaviors, similar to successful backdoor attacks. To mitigate this
vulnerability, we propose Random Noise Augmentation -- a model and method
agnostic approach with theoretical guarantees for improving the robustness of
RM methods. Extensive experiments demonstrate that RNA significantly improves
the robustness of RM models while enhancing the unlearning performances.
arXiv ID: http://arxiv.org/abs/2501.19202v1
URL: http://arxiv.org/abs/2501.19202v1

================================================================================

Title: Efficient Reasoning with Hidden Thinking
Authors: Xuan Shen, Yizhou Wang, Xiangxi Shi, Yanzhi Wang, Pu Zhao, Jiuxiang Gu
Abstract: Chain-of-Thought (CoT) reasoning has become a powerful framework for
improving complex problem-solving capabilities in Multimodal Large Language
Models (MLLMs). However, the verbose nature of textual reasoning introduces
significant inefficiencies. In this work, we propose $\textbf{Heima}$ (as
hidden llama), an efficient reasoning framework that leverages reasoning CoTs
at hidden latent space. We design the Heima Encoder to condense each
intermediate CoT into a compact, higher-level hidden representation using a
single thinking token, effectively minimizing verbosity and reducing the
overall number of tokens required during the reasoning process. Meanwhile, we
design corresponding Heima Decoder with traditional Large Language Models
(LLMs) to adaptively interpret the hidden representations into variable-length
textual sequence, reconstructing reasoning processes that closely resemble the
original CoTs. Experimental results across diverse reasoning MLLM benchmarks
demonstrate that Heima model achieves higher generation efficiency while
maintaining or even better zero-shot task accuracy. Moreover, the effective
reconstruction of multimodal reasoning processes with Heima Decoder validates
both the robustness and interpretability of our approach.
arXiv ID: http://arxiv.org/abs/2501.19201v1
URL: http://arxiv.org/abs/2501.19201v1

================================================================================

Title: Enhancing Model Defense Against Jailbreaks with Proactive Safety
  Reasoning
Authors: Xianglin Yang, Gelei Deng, Jieming Shi, Tianwei Zhang, Jin Song Dong
Abstract: Large language models (LLMs) are vital for a wide range of applications yet
remain susceptible to jailbreak threats, which could lead to the generation of
inappropriate responses. Conventional defenses, such as refusal and adversarial
training, often fail to cover corner cases or rare domains, leaving LLMs still
vulnerable to more sophisticated attacks. We propose a novel defense strategy,
Safety Chain-of-Thought (SCoT), which harnesses the enhanced \textit{reasoning
capabilities} of LLMs for proactive assessment of harmful inputs, rather than
simply blocking them. SCoT augments any refusal training datasets to critically
analyze the intent behind each request before generating answers. By employing
proactive reasoning, SCoT enhances the generalization of LLMs across varied
harmful queries and scenarios not covered in the safety alignment corpus.
Additionally, it generates detailed refusals specifying the rules violated.
Comparative evaluations show that SCoT significantly surpasses existing
defenses, reducing vulnerability to out-of-distribution issues and adversarial
manipulations while maintaining strong general capabilities.
arXiv ID: http://arxiv.org/abs/2501.19180v1
URL: http://arxiv.org/abs/2501.19180v1

================================================================================

Title: Position: Contextual Integrity Washing for Language Models
Authors: Yan Shvartzshnaider, Vasisht Duddu
Abstract: Machine learning community is discovering Contextual Integrity (CI) as a
useful framework to assess the privacy implications of large language models
(LLMs). This is an encouraging development. The CI theory emphasizes sharing
information in accordance with privacy norms and can bridge the social, legal,
political, and technical aspects essential for evaluating privacy in LLMs.
However, this is also a good point to reflect on use of CI for LLMs. This
position paper argues that existing literature adopts CI for LLMs without
embracing the theory's fundamental tenets, essentially amounting to a form of
"CI-washing." CI-washing could lead to incorrect conclusions and flawed
privacy-preserving designs. We clarify the four fundamental tenets of CI
theory, systematize prior work on whether they deviate from these tenets, and
highlight overlooked issues in experimental hygiene for LLMs (e.g., prompt
sensitivity, positional bias).
arXiv ID: http://arxiv.org/abs/2501.19173v1
URL: http://arxiv.org/abs/2501.19173v1

================================================================================

Title: Poison as Cure: Visual Noise for Mitigating Object Hallucinations in
  LVMs
Authors: Kejia Zhang, Keda Tao, Jiasheng Tang, Huan Wang
Abstract: Large vision-language models (LVMs) extend large language models (LLMs) with
visual perception capabilities, enabling them to process and interpret visual
information. A major challenge compromising their reliability is object
hallucination that LVMs may generate plausible but factually inaccurate
information. We propose a novel visual adversarial perturbation (VAP) method to
mitigate this hallucination issue. VAP alleviates LVM hallucination by applying
strategically optimized visual noise without altering the base model. Our
approach formulates hallucination suppression as an optimization problem,
leveraging adversarial strategies to generate beneficial visual perturbations
that enhance the model's factual grounding and reduce parametric knowledge
bias. Extensive experimental results demonstrate that our method consistently
reduces object hallucinations across 8 state-of-the-art LVMs, validating its
efficacy across diverse evaluations.
arXiv ID: http://arxiv.org/abs/2501.19164v1
URL: http://arxiv.org/abs/2501.19164v1

================================================================================

Title: A Tensor-Train Decomposition based Compression of LLMs on Group Vector
  Systolic Accelerator
Authors: Sixiao Huang, Tintin Wang, Ang Li, Ao Shen, Kai Li, Keyao Jiang, Mingqiang Huang, Hao Yu
Abstract: Large language models (LLMs) are both storage-intensive and
computation-intensive, posing significant challenges when deployed on
resource-constrained hardware. As linear layers in LLMs are mainly resource
consuming parts, this paper develops a tensor-train decomposition (TTD) for
LLMs with a further hardware implementation on FPGA. TTD compression is applied
to the linear layers in ChatGLM3-6B and LLaMA2-7B models with compression
ratios (CRs) for the whole network 1.94$\times$ and 1.60$\times$, respectively.
The compressed LLMs are further implemented on FPGA hardware within a highly
efficient group vector systolic array (GVSA) architecture, which has DSP-shared
parallel vector PEs for TTD inference, as well as optimized data communication
in the accelerator. Experimental results show that the corresponding TTD based
LLM accelerator implemented on FPGA achieves 1.45$\times$ and 1.57$\times$
reduction in first token delay for ChatGLM3-6B and LLaMA2-7B models,
respectively.
arXiv ID: http://arxiv.org/abs/2501.19135v1
URL: http://arxiv.org/abs/2501.19135v1

================================================================================

Title: Brain-inspired sparse training enables Transformers and LLMs to perform
  as fully connected
Authors: Yingtao Zhang, Jialin Zhao, Wenjing Wu, Ziheng Liao, Umberto Michieli, Carlo Vittorio Cannistraci
Abstract: This study aims to enlarge our current knowledge on application of
brain-inspired network science principles for training artificial neural
networks (ANNs) with sparse connectivity. Dynamic sparse training (DST) can
reduce the computational demands in ANNs, but faces difficulties to keep peak
performance at high sparsity levels. The Cannistraci-Hebb training (CHT) is a
brain-inspired method for growing connectivity in DST. CHT leverages a
gradient-free, topology-driven link regrowth, which has shown ultra-sparse (1%
connectivity or lower) advantage across various tasks compared to fully
connected networks. Yet, CHT suffers two main drawbacks: (i) its time
complexity is O(Nd^3) - N node network size, d node degree - hence it can apply
only to ultra-sparse networks. (ii) it selects top link prediction scores,
which is inappropriate for the early training epochs, when the network presents
unreliable connections. We propose a GPU-friendly approximation of the CH link
predictor, which reduces the computational complexity to O(N^3), enabling a
fast implementation of CHT in large-scale models. We introduce the
Cannistraci-Hebb training soft rule (CHTs), which adopts a strategy for
sampling connections in both link removal and regrowth, balancing the
exploration and exploitation of network topology. To improve performance, we
integrate CHTs with a sigmoid gradual density decay (CHTss). Empirical results
show that, using 1% of connections, CHTs outperforms fully connected networks
in MLP on visual classification tasks, compressing some networks to &lt; 30%
nodes. Using 5% of the connections, CHTss outperforms fully connected networks
in two Transformer-based machine translation tasks. Using 30% of the
connections, CHTss achieves superior performance compared to other dynamic
sparse training methods in language modeling, and it surpasses the fully
connected counterpart in zero-shot evaluations.
arXiv ID: http://arxiv.org/abs/2501.19107v1
URL: http://arxiv.org/abs/2501.19107v1

================================================================================

Title: Unraveling Zeroth-Order Optimization through the Lens of Low-Dimensional
  Structured Perturbations
Authors: Sihwan Park, Jihun Yun, SungYub Kim, Souvik Kundu, Eunho Yang
Abstract: Zeroth-order (ZO) optimization has emerged as a promising alternative to
gradient-based backpropagation methods, particularly for black-box optimization
and large language model (LLM) fine-tuning. However, ZO methods suffer from
slow convergence due to high-variance stochastic gradient estimators. While
structured perturbations, such as sparsity and low-rank constraints, have been
explored to mitigate these issues, their effectiveness remains highly
under-explored. In this work, we develop a unified theoretical framework that
analyzes both the convergence and generalization properties of ZO optimization
under structured perturbations. We show that high dimensionality is the primary
bottleneck and introduce the notions of \textit{stable rank} and
\textit{effective overlap} to explain how structured perturbations reduce
gradient noise and accelerate convergence. Using the uniform stability under
our framework, we then provide the first theoretical justification for why
these perturbations enhance generalization. Additionally, through empirical
analysis, we identify that \textbf{block coordinate descent} (BCD) to be an
effective structured perturbation method. Extensive experiments show that,
compared to existing alternatives, memory-efficient ZO (MeZO) with BCD
(\textit{MeZO-BCD}) can provide improved converge with a faster wall-clock
time/iteration by up to $\times\textbf{2.09}$ while yielding similar or better
accuracy.
arXiv ID: http://arxiv.org/abs/2501.19099v1
URL: http://arxiv.org/abs/2501.19099v1

================================================================================

Title: Improving Low-Resource Sequence Labeling with Knowledge Fusion and
  Contextual Label Explanations
Authors: Peichao Lai, Jiaxin Gan, Feiyang Ye, Yilei Wang, Bin Cui
Abstract: Sequence labeling remains a significant challenge in low-resource,
domain-specific scenarios, particularly for character-dense languages like
Chinese. Existing methods primarily focus on enhancing model comprehension and
improving data diversity to boost performance. However, these approaches still
struggle with inadequate model applicability and semantic distribution biases
in domain-specific contexts. To overcome these limitations, we propose a novel
framework that combines an LLM-based knowledge enhancement workflow with a
span-based Knowledge Fusion for Rich and Efficient Extraction (KnowFREE) model.
Our workflow employs explanation prompts to generate precise contextual
interpretations of target entities, effectively mitigating semantic biases and
enriching the model's contextual understanding. The KnowFREE model further
integrates extension label features, enabling efficient nested entity
extraction without relying on external knowledge during inference. Experiments
on multiple Chinese domain-specific sequence labeling datasets demonstrate that
our approach achieves state-of-the-art performance, effectively addressing the
challenges posed by low-resource settings.
arXiv ID: http://arxiv.org/abs/2501.19093v1
URL: http://arxiv.org/abs/2501.19093v1

================================================================================

Title: Pivoting Factorization: A Compact Meta Low-Rank Representation of
  Sparsity for Efficient Inference in Large Language Models
Authors: Jialin Zhao, Yingtao Zhang, Carlo Vittorio Cannistraci
Abstract: The rapid growth of Large Language Models has driven demand for effective
model compression techniques to reduce memory and computation costs. Low-rank
pruning has gained attention for its tensor coherence and GPU compatibility
across all densities. However, low-rank pruning has struggled to match the
performance of semi-structured pruning, often doubling perplexity (PPL) at
similar densities. In this paper, we propose Pivoting Factorization (PIFA), a
novel lossless meta low-rank representation that unsupervisedly learns a
compact form of any low-rank representation, effectively eliminating redundant
information. PIFA identifies pivot rows (linearly independent rows) and
expresses non-pivot rows as linear combinations, achieving an additional 24.2\%
memory savings and 24.6\% faster inference over low-rank layers at r/d = 0.5,
thereby significantly enhancing performance at the same density. To mitigate
the performance degradation caused by low-rank pruning, we introduce a novel,
retraining-free low-rank reconstruction method that minimizes error
accumulation (M). MPIFA, combining M and PIFA into an end-to-end framework,
significantly outperforms existing low-rank pruning methods and, for the first
time, achieves performance comparable to semi-structured pruning, while
surpassing it in GPU efficiency and compatibility.
arXiv ID: http://arxiv.org/abs/2501.19090v1
URL: http://arxiv.org/abs/2501.19090v1

================================================================================

Title: Enhancing Code Generation for Low-Resource Languages: No Silver Bullet
Authors: Alessandro Giagnorio, Alberto Martin-Lopez, Gabriele Bavota
Abstract: The advent of Large Language Models (LLMs) has significantly advanced the
field of automated code generation. LLMs rely on large and diverse datasets to
learn syntax, semantics, and usage patterns of programming languages. For
low-resource languages (i.e., niche programming languages characterized by the
scarcity of training data), the limited availability of such data hampers the
models' ability to generalize effectively, resulting in poorer code generation
performance as compared to high-resource languages. For this reason, there is a
quest for techniques able to close this performance gap. We present an
empirical study investigating the effectiveness of several approaches for
boosting LLMs' performance on low-resource languages, namely: (i) a classic
fine-tuning, which is however capped in size by the scarcity of training data;
(ii) three variants of in-context learning, with prompts crafted to provide the
LLM with additional information about the low-resource language (e.g., few-shot
examples showcasing features of the targeted language); and (iii) a
pre-training objective teaching the model how to translate between high- and
low-resource languages. The context of our study are two low-resource languages
(R and Racket) and six LLMs having different architectures and sizes. Our
findings reveal that a fine-tuning is usually the best choice for smaller LLMs,
possibly due to the fact that even a small dataset is sufficient to train their
limited number of parameters. With the increase in size of the models,
in-context learning becomes more and more effective, representing a safe and
cheap bet (i.e., it always helps, but with different magnitudes). Differently,
very large LLMs may deteriorate their performance on low-resource languages
when fine-tuning is performed, possibly due to the lack of enough data needed
to effectively update their weights.
arXiv ID: http://arxiv.org/abs/2501.19085v1
URL: http://arxiv.org/abs/2501.19085v1

================================================================================

Title: TeZO: Empowering the Low-Rankness on the Temporal Dimension in the
  Zeroth-Order Optimization for Fine-tuning LLMs
Authors: Yan Sun, Tiansheng Huang, Liang Ding, Li Shen, Dacheng Tao
Abstract: Zeroth-order optimization (ZO) has demonstrated remarkable promise in
efficient fine-tuning tasks for Large Language Models (LLMs). In particular,
recent advances incorporate the low-rankness of gradients, introducing low-rank
ZO estimators to further reduce GPU memory consumption. However, most existing
works focus solely on the low-rankness of each individual gradient, overlooking
a broader property shared by all gradients throughout the training, i.e., all
gradients approximately reside within a similar subspace. In this paper, we
consider two factors together and propose a novel low-rank ZO estimator, TeZO,
which captures the low-rankness across both the model and temporal dimension.
Specifically, we represent ZO perturbations along the temporal dimension as a
3D tensor and employ Canonical Polyadic Decomposition (CPD) to extract each
low-rank 2D matrix, significantly reducing the training cost. TeZO can also be
easily extended to the Adam variant while consuming less memory than MeZO-SGD,
and requiring about only 35% memory of MeZO-Adam. Both comprehensive
theoretical analysis and extensive experimental research have validated its
efficiency, achieving SOTA-comparable results with lower overhead of time and
memory.
arXiv ID: http://arxiv.org/abs/2501.19057v1
URL: http://arxiv.org/abs/2501.19057v1

================================================================================

Title: Enabling Autonomic Microservice Management through Self-Learning Agents
Authors: Fenglin Yu, Fangkai Yang, Xiaoting Qin, Zhiyang Zhang, Jue Zhang, Qingwei Lin, Hongyu Zhang, Yingnong Dang, Saravan Rajmohan, Dongmei Zhang, Qi Zhang
Abstract: The increasing complexity of modern software systems necessitates robust
autonomic self-management capabilities. While Large Language Models (LLMs)
demonstrate potential in this domain, they often face challenges in adapting
their general knowledge to specific service contexts. To address this
limitation, we propose ServiceOdyssey, a self-learning agent system that
autonomously manages microservices without requiring prior knowledge of
service-specific configurations. By leveraging curriculum learning principles
and iterative exploration, ServiceOdyssey progressively develops a deep
understanding of operational environments, reducing dependence on human input
or static documentation. A prototype built with the Sock Shop microservice
demonstrates the potential of this approach for autonomic microservice
management.
arXiv ID: http://arxiv.org/abs/2501.19056v1
URL: http://arxiv.org/abs/2501.19056v1

================================================================================

Title: Text-to-CAD Generation Through Infusing Visual Feedback in Large
  Language Models
Authors: Ruiyu Wang, Yu Yuan, Shizhao Sun, Jiang Bian
Abstract: Creating Computer-Aided Design (CAD) models requires significant expertise
and effort. Text-to-CAD, which converts textual descriptions into CAD
parametric sequences, is crucial in streamlining this process. Recent studies
have utilized ground-truth parametric sequences, known as sequential signals,
as supervision to achieve this goal. However, CAD models are inherently
multimodal, comprising parametric sequences and corresponding rendered visual
objects. Besides,the rendering process from parametric sequences to visual
objects is many-to-one. Therefore, both sequential and visual signals are
critical for effective training. In this work, we introduce CADFusion, a
framework that uses Large Language Models (LLMs) as the backbone and alternates
between two training stages: the sequential learning (SL) stage and the visual
feedback (VF) stage. In the SL stage, we train LLMs using ground-truth
parametric sequences, enabling the generation of logically coherent parametric
sequences. In the VF stage, we reward parametric sequences that render into
visually preferred objects and penalize those that do not, allowing LLMs to
learn how rendered visual objects are perceived and evaluated. These two stages
alternate throughout the training, ensuring balanced learning and preserving
benefits of both signals. Experiments demonstrate that CADFusion significantly
improves performance, both qualitatively and quantitatively.
arXiv ID: http://arxiv.org/abs/2501.19054v1
URL: http://arxiv.org/abs/2501.19054v1

================================================================================

Title: Towards the Worst-case Robustness of Large Language Models
Authors: Huanran Chen, Yinpeng Dong, Zeming Wei, Hang Su, Jun Zhu
Abstract: Recent studies have revealed the vulnerability of Large Language Models
(LLMs) to adversarial attacks, where the adversary crafts specific input
sequences to induce harmful, violent, private, or incorrect outputs. Although
various defenses have been proposed, they have not been evaluated by strong
adaptive attacks, leaving the worst-case robustness of LLMs still intractable.
By developing a stronger white-box attack, our evaluation results indicate that
most typical defenses achieve nearly 0\% robustness.To solve this, we propose
\textit{DiffTextPure}, a general defense that diffuses the (adversarial) input
prompt using any pre-defined smoothing distribution, and purifies the diffused
input using a pre-trained language model. Theoretically, we derive tight
robustness lower bounds for all smoothing distributions using Fractal Knapsack
or 0-1 Knapsack solvers. Under this framework, we certify the robustness of a
specific case -- smoothing LLMs using a uniform kernel -- against \textit{any
possible attack} with an average $\ell_0$ perturbation of 2.02 or an average
suffix length of 6.41.
arXiv ID: http://arxiv.org/abs/2501.19040v1
URL: http://arxiv.org/abs/2501.19040v1

================================================================================

Title: Beyond Token Compression: A Training-Free Reduction Framework for
  Efficient Visual Processing in MLLMs
Authors: Hongliang Li, Jiaxin Zhang, Wenhui Liao, Dezhi Peng, Kai Ding, Lianwen Jin
Abstract: Multimodal Large Language Models (MLLMs) are typically based on decoder-only
or cross-attention architectures. While decoder-only MLLMs outperform their
cross-attention counterparts, they require significantly higher computational
resources due to extensive self-attention and FFN operations on visual tokens.
This raises the question: can we eliminate these expensive operations while
maintaining the performance? To this end, we present a novel analysis framework
to investigate the necessity of these costly operations in decoder-only MLLMs.
Our framework introduces two key innovations: (1) Hollow Attention, which
limits visual token interactions to local attention while maintaining
visual-text associations, and (2) Probe-Activated Dynamic FFN, which
selectively activates FFN parameters for visual tokens. Both methods do not
require fine-tuning, which significantly enhances analysis efficiency. To
assess the impact of applying these reductions across different proportions of
layers, we developed a greedy search method that significantly narrows the
search space. Experiments on state-of-the-art MLLMs reveal that applying our
reductions to approximately half of the layers not only maintains but sometimes
improves model performance, indicating significant computational redundancy in
current architectures. Additionally, our method is orthogonal to existing token
compression techniques, allowing for further combination to achieve greater
computational reduction. Our findings may provide valuable insights for the
design of more efficient future MLLMs. Our code will be publicly available at
https://github.com/L-Hugh/Beyond-Token-Compression.
arXiv ID: http://arxiv.org/abs/2501.19036v1
URL: http://arxiv.org/abs/2501.19036v1

================================================================================

Title: Calling a Spade a Heart: Gaslighting Multimodal Large Language Models
  via Negation
Authors: Bin Zhu, Hui yan Qi, Yinxuan Gui, Jingjing Chen, Chong-Wah Ngo, Ee Peng Lim
Abstract: Multimodal Large Language Models (MLLMs) have exhibited remarkable
advancements in integrating different modalities, excelling in complex
understanding and generation tasks. Despite their success, MLLMs remain
vulnerable to conversational adversarial inputs, particularly negation
arguments. This paper systematically evaluates state-of-the-art MLLMs across
diverse benchmarks, revealing significant performance drops when negation
arguments are introduced to initially correct responses. We show critical
vulnerabilities in the reasoning and alignment mechanisms of these models.
Proprietary models such as GPT-4o and Claude-3.5-Sonnet demonstrate better
resilience compared to open-source counterparts like Qwen2-VL and LLaVA.
However, all evaluated MLLMs struggle to maintain logical consistency under
negation arguments during conversation. This paper aims to offer valuable
insights for improving the robustness of MLLMs against adversarial inputs,
contributing to the development of more reliable and trustworthy multimodal AI
systems.
arXiv ID: http://arxiv.org/abs/2501.19017v1
URL: http://arxiv.org/abs/2501.19017v1

================================================================================

Title: Importing Phantoms: Measuring LLM Package Hallucination Vulnerabilities
Authors: Arjun Krishna, Erick Galinkin, Leon Derczynski, Jeffrey Martin
Abstract: Large Language Models (LLMs) have become an essential tool in the
programmer's toolkit, but their tendency to hallucinate code can be used by
malicious actors to introduce vulnerabilities to broad swathes of the software
supply chain. In this work, we analyze package hallucination behaviour in LLMs
across popular programming languages examining both existing package references
and fictional dependencies. By analyzing this package hallucination behaviour
we find potential attacks and suggest defensive strategies to defend against
these attacks. We discover that package hallucination rate is predicated not
only on model choice, but also programming language, model size, and
specificity of the coding task request. The Pareto optimality boundary between
code generation performance and package hallucination is sparsely populated,
suggesting that coding models are not being optimized for secure code.
Additionally, we find an inverse correlation between package hallucination rate
and the HumanEval coding benchmark, offering a heuristic for evaluating the
propensity of a model to hallucinate packages. Our metrics, findings and
analyses provide a base for future models, securing AI-assisted software
development workflows against package supply chain attacks.
arXiv ID: http://arxiv.org/abs/2501.19012v1
URL: http://arxiv.org/abs/2501.19012v1

================================================================================

Title: Symmetric Pruning of Large Language Models
Authors: Kai Yi, Peter Richtárik
Abstract: Popular post-training pruning methods such as Wanda and RIA are known for
their simple, yet effective, designs that have shown exceptional empirical
performance. Wanda optimizes performance through calibrated activations during
pruning, while RIA emphasizes the relative, rather than absolute, importance of
weight elements. Despite their practical success, a thorough theoretical
foundation explaining these outcomes has been lacking. This paper introduces
new theoretical insights that redefine the standard minimization objective for
pruning, offering a deeper understanding of the factors contributing to their
success. Our study extends beyond these insights by proposing complementary
strategies that consider both input activations and weight significance. We
validate these approaches through rigorous experiments, demonstrating
substantial enhancements over existing methods. Furthermore, we introduce a
novel training-free fine-tuning approach $R^2$-DSnoT that incorporates relative
weight importance and a regularized decision boundary within a dynamic
pruning-and-growing framework, significantly outperforming strong baselines and
establishing a new state of the art.
arXiv ID: http://arxiv.org/abs/2501.18980v1
URL: http://arxiv.org/abs/2501.18980v1

================================================================================

Title: Spend Wisely: Maximizing Post-Training Gains in Iterative Synthetic Data
  Boostrapping
Authors: Pu Yang, Yunzhen Feng, Ziyuan Chen, Yuhang Wu, Zhuoyuan Li
Abstract: Modern foundation models often undergo iterative ``bootstrapping'' in their
post-training phase: a model generates synthetic data, an external verifier
filters out low-quality samples, and the high-quality subset is used for
further fine-tuning. Over multiple iterations, the model's performance
improves--raising a crucial question: how should the total budget on generation
and training be allocated across iterations to maximize final performance? In
this work, we develop a theoretical framework to analyze budget allocation
strategies. Specifically, we show that constant policies fail to converge with
high probability, while increasing policies--particularly exponential growth
policies--exhibit significant theoretical advantages. Experiments on image
denoising with diffusion probabilistic models and math reasoning with large
language models show that both exponential and polynomial growth policies
consistently outperform constant policies, with exponential policies often
providing more stable performance.
arXiv ID: http://arxiv.org/abs/2501.18962v1
URL: http://arxiv.org/abs/2501.18962v1

================================================================================

Title: Intrinsic Tensor Field Propagation in Large Language Models: A Novel
  Approach to Contextual Information Flow
Authors: Alfred Bexley, Lukas Radcliffe, Giles Weatherstone, Joseph Sakau
Abstract: Context propagation remains a central challenge in language model
architectures, particularly in tasks requiring the retention of long-range
dependencies. Conventional attention mechanisms, while effective in many
applications, exhibit limitations in maintaining coherent contextual
representations over extended sequences due to their reliance on discrete token
interactions. A novel approach is introduced through the formulation of
Intrinsic Tensor Field Propagation (ITFP), which models contextual
relationships as continuous tensor fields distributed across token embeddings.
The propagation dynamics are governed through differential equations that
enable a structured flow of contextual information, augmenting the standard
attention mechanism to enhance coherence and recall. A series of experiments
conducted on an open-source transformer-based model demonstrate that ITFP
provides measurable improvements in contextual retention, dependency
resolution, and inference stability across various linguistic structures.
Comparisons with baseline models reveal a reduction in syntactic
inconsistencies and factual errors, while ablation studies indicate that the
choice of propagation depth and integration strength significantly impacts
model performance. Additional evaluations assessing domain generalization
suggest that ITFP effectively adapts across different text genres, reinforcing
its applicability beyond conventional language modeling tasks. Although
computational trade-offs are introduced through the inclusion of tensor field
computations, empirical findings suggest that the benefits in accuracy and
coherence outweigh the increased processing demands.
arXiv ID: http://arxiv.org/abs/2501.18957v1
URL: http://arxiv.org/abs/2501.18957v1

================================================================================

Title: LLMDet: Learning Strong Open-Vocabulary Object Detectors under the
  Supervision of Large Language Models
Authors: Shenghao Fu, Qize Yang, Qijie Mo, Junkai Yan, Xihan Wei, Jingke Meng, Xiaohua Xie, Wei-Shi Zheng
Abstract: Recent open-vocabulary detectors achieve promising performance with abundant
region-level annotated data. In this work, we show that an open-vocabulary
detector co-training with a large language model by generating image-level
detailed captions for each image can further improve performance. To achieve
the goal, we first collect a dataset, GroundingCap-1M, wherein each image is
accompanied by associated grounding labels and an image-level detailed caption.
With this dataset, we finetune an open-vocabulary detector with training
objectives including a standard grounding loss and a caption generation loss.
We take advantage of a large language model to generate both region-level short
captions for each region of interest and image-level long captions for the
whole image. Under the supervision of the large language model, the resulting
detector, LLMDet, outperforms the baseline by a clear margin, enjoying superior
open-vocabulary ability. Further, we show that the improved LLMDet can in turn
build a stronger large multi-modal model, achieving mutual benefits. The code,
model, and dataset is available at https://github.com/iSEE-Laboratory/LLMDet.
arXiv ID: http://arxiv.org/abs/2501.18954v1
URL: http://arxiv.org/abs/2501.18954v1

================================================================================

